# Проект по анализу данных. Этапы работы с данными
Работа с данными в проекте анализа состоит из четырёх ключевых этапов:

## 1. Идентификация данных

**Определение доступных и подходящих данных.**

### Что делаем:
- Определяем, какие данные нужны для решения задачи.
- Ищем источники: базы данных (БД), API, файлы, логи, CRM, Excel.
- Оцениваем:
  - Достаточно ли данных?
  - Релевантны ли они задаче?
  - Есть ли ограничения (бюджет, сроки, доступ)?

###  Пример:
**Задача:** предсказать отток клиентов.  
→ Нам нужны:
1. Данные о покупках (сумма, частота)
2. Данные о демографии (возраст, пол)
3. Данные о поведении на сайте (время на сайте, просмотренные страницы)

→ Идентифицируем источники:
- БД продаж → таблица `transactions`
- CRM → таблица `clients`
- Google Analytics → логи сессий

>  **Важно:** если нужных данных нет — проект может быть отменён или перепланирован.

---

## 2. Извлечение данных

**Извлечение данных из источников с использованием существующих инструментов.**

### Что делаем:
- Подключаемся к источникам (SQL, API, файлы).
- Используем инструменты:
  - Python: `pandas`, `SQLAlchemy`
  - SQL-запросы
  - ETL-инструменты: Apache Airflow, Talend
- Выгружаем данные в единый формат (например, CSV или pandas DataFrame).

---

## 3. Подготовка данных

**Определение и осуществление необходимых шагов по трансформации и очистке данных.**

>  Это **главная часть работы** — занимает до 80% времени проекта.  
> Состоит из трёх подэтапов:

### 3.1. Выделение признаков (Feature Extraction)

**Создание новых признаков из исходных данных.**

#### Что делаем:
- Генерируем новые переменные:
  - Возраст → возрастная группа
  - Дата регистрации → количество дней с регистрации
  - Сумма покупок → средний чек за месяц
  - Текст сообщения → количество слов, наличие ключевых слов

>  **Цель:** сделать данные более информативными для модели.

---

### 3.2. Отбор признаков (Feature Selection)

**Выбор наиболее значимых признаков для модели.**

#### Что делаем:
- Удаляем ненужные столбцы (ID, дубликаты, константы).
- Используем методы:
  - Анализ корреляции (удаляем сильно коррелирующие признаки)
  - Feature importance из Random Forest
  - Lasso-регрессия (автоматически обнуляет незначимые признаки)

>  **Цель:** уменьшить размерность, улучшить качество модели, ускорить обучение.

---

### 3.3. Преобразование признаков (Feature Transformation)

**Изменение формы признаков для лучшей работы модели.**

#### Что делаем:
- Кодирование категорий:
  - One-Hot Encoding
  - Label Encoding
- Масштабирование:
  - `StandardScaler`
  - `MinMaxScaler`
- Другие преобразования:
  - Логарифмирование
  - Биннинг (разбиение на интервалы)
  - Нормализация

>  **Цель:** сделать данные пригодными для модели — ведь многие алгоритмы (например, KNN, SVM) чувствительны к масштабу признаков.

---

## 4. Агрегация данных

**Проведение необходимых расчётов и расчёт агрегатов.**

### Что делаем:
- Группировка данных: по клиенту, по месяцу, по категории
- Расчёт агрегатов:
  - Сумма
  - Среднее
  - Количество
  - Максимум / минимум
- Создание сводных таблиц (`pivot tables`)

>  **Цель:** получить сводную информацию, которая поможет в анализе и принятии решений.
